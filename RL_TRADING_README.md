# RL Trading System

This repository now includes a comprehensive Reinforcement Learning (RL) trading system that allows you to train intelligent agents for cryptocurrency algorithmic trading. The system integrates seamlessly with the existing trading infrastructure and provides state-of-the-art RL capabilities.

## üöÄ Features

### RL Agent Training
- **Multiple Algorithms**: PPO, DQN, A2C with hyperparameter tuning
- **Ensemble Methods**: Combine multiple agents for robust performance
- **Advanced Environment**: Comprehensive market state representation with technical indicators
- **Risk-Adjusted Rewards**: Sharpe ratio optimization with drawdown penalties

### Backtesting Framework
- **Strategy Comparison**: Compare RL agents vs traditional strategies
- **Performance Metrics**: Sharpe ratio, Calmar ratio, VaR, CVaR, and more
- **Walk-Forward Analysis**: Robust validation methodology
- **Risk Management**: Transaction costs, slippage, and position sizing

### Data Management
- **Binance Integration**: Automated historical data collection
- **Real-time Streaming**: Live market data for training and trading
- **Multi-Asset Support**: Portfolio optimization across multiple crypto pairs
- **Quality Assurance**: Data validation and preprocessing pipeline

### Deployment
- **Live Trading Integration**: Deploy RL agents as trading strategies
- **Strategy Manager**: Run multiple RL strategies simultaneously
- **Monitoring**: Real-time performance tracking and observability
- **CLI Interface**: Easy command-line access to all functionality

## üìÅ File Structure

```
crypto/
‚îú‚îÄ‚îÄ rl_environment.py      # Trading environment for RL training
‚îú‚îÄ‚îÄ rl_agents.py           # RL agent implementations (PPO, DQN, A2C)
‚îú‚îÄ‚îÄ rl_trainer.py          # Training pipeline and RL strategy integration
‚îú‚îÄ‚îÄ rl_cli.py              # Command-line interface
‚îú‚îÄ‚îÄ backtesting.py         # Comprehensive backtesting framework
‚îú‚îÄ‚îÄ data_collector.py      # Data collection and preprocessing
‚îî‚îÄ‚îÄ strategy_manager.py    # Updated with RL strategy support

examples/
‚îî‚îÄ‚îÄ rl_trading_example.py  # Complete usage examples

models/                     # Trained RL models
data/                      # Historical market data
results/                   # Backtest results and analysis
```

## üîß Installation

1. **Install Dependencies**:
```bash
pip install -r requirements.txt
```

The system includes new ML/RL dependencies:
- `stable-baselines3`: RL algorithms
- `gym`: Environment framework  
- `torch`: Neural networks
- `pandas`, `scikit-learn`: Data processing
- `matplotlib`: Visualization

2. **Optional Dependencies**:
```bash
pip install optuna  # For hyperparameter optimization
pip install tensorboard  # For training visualization
```

## üöÄ Quick Start

### 1. Collect Training Data

```bash
# Collect 2 years of hourly data for BTC and ETH
python crypto/rl_cli.py collect-data BTCUSDT ETHUSDT --days-back 730 --intervals 1h
```

### 2. Train an RL Agent

```bash
# Train a PPO agent on BTC data
python crypto/rl_cli.py train btc_ppo_agent BTCUSDT --algorithm PPO --timesteps 100000
```

### 3. Backtest the Agent

```bash
# Backtest the trained agent
python crypto/rl_cli.py backtest BTCUSDT --agent-names btc_ppo_agent --test-start 2023-06-01
```

### 4. Compare with Traditional Strategies

```bash
# Compare RL agent performance vs traditional strategies
python crypto/rl_cli.py compare BTCUSDT --agent-names btc_ppo_agent --test-start 2023-06-01
```

### 5. Deploy for Live Trading

```python
from crypto.rl_trainer import RLTrainingPipeline
from crypto.strategy_manager import StrategyManager

# Load trained agent
pipeline = RLTrainingPipeline()
pipeline.load_agent('btc_ppo_agent', './models/btc_ppo_agent_final.zip', 'PPO')

# Deploy with strategy manager
manager = StrategyManager(client, notifier)
config = {'agent': pipeline.agents['btc_ppo_agent']}
strategy_id = manager.create_strategy('RLStrategy', 'BTC', 'USDT', config)
manager.start_strategy(strategy_id)
```

## üìö Detailed Usage

### Training Configuration

```python
from crypto.rl_trainer import TrainingConfig

config = TrainingConfig(
    algorithm="PPO",           # PPO, DQN, or A2C
    total_timesteps=100000,    # Training duration
    learning_rate=3e-4,        # Learning rate
    batch_size=64,             # Batch size
    gamma=0.99,                # Discount factor
    # ... other hyperparameters
)
```

### Environment Configuration

The RL environment includes:
- **State Space**: OHLCV data + 20+ technical indicators + portfolio state
- **Action Space**: HOLD (0), BUY (1), SELL (2)
- **Reward Function**: Portfolio returns + Sharpe ratio bonus - transaction costs - drawdown penalties

### Advanced Features

#### Ensemble Training
```bash
# Train ensemble of multiple algorithms
python crypto/rl_cli.py train-ensemble btc_ensemble BTCUSDT --algorithms PPO DQN A2C
```

#### Hyperparameter Optimization
```bash
# Optimize hyperparameters with Optuna
python crypto/rl_cli.py optimize BTCUSDT --algorithm PPO --n-trials 50
```

#### Multi-Asset Training
```python
# Train agents on multiple cryptocurrency pairs
symbols = ['BTCUSDT', 'ETHUSDT', 'ADAUSDT', 'DOTUSDT']
datasets = pipeline.data_collector.create_multi_asset_dataset(symbols)
```

## üìä Performance Analysis

The system provides comprehensive performance metrics:

- **Return Metrics**: Total return, annualized return, CAGR
- **Risk Metrics**: Volatility, Sharpe ratio, Sortino ratio, Calmar ratio
- **Drawdown Analysis**: Maximum drawdown, recovery time
- **Trade Analysis**: Win rate, profit factor, average trade duration
- **Risk Measures**: VaR, CVaR at various confidence levels

### Example Results

```
Strategy Comparison Results:
================================================================
Strategy                Total Return  Sharpe Ratio  Max Drawdown
btc_ppo_agent          45.2%         1.84          -12.3%
btc_ensemble           52.1%         2.01          -9.8%
RSI                    23.4%         1.12          -18.7%
MeanReversionStrategy  31.2%         1.45          -15.2%
Buy and Hold           38.9%         1.23          -22.1%
```

## üéØ RL Agent Architecture

### State Representation
- **Price Features**: Normalized OHLCV data (30-period lookback)
- **Technical Indicators**: RSI, MACD, Bollinger Bands, ATR, etc.
- **Portfolio State**: Current balance, position, unrealized PnL
- **Market Conditions**: Volume profile, volatility measures

### Reward Function
```python
reward = portfolio_return * scaling_factor 
       + sharpe_bonus * 0.1
       - drawdown_penalty * 10  
       - transaction_cost * 0.01
```

### Neural Network Architecture
- **PPO**: Actor-Critic with shared features
- **DQN**: Deep Q-Network with experience replay
- **A2C**: Advantage Actor-Critic with entropy regularization

## üîß Customization

### Custom Reward Functions
```python
class CustomTradingEnvironment(CryptoTradingEnvironment):
    def _calculate_reward(self, prev_portfolio_value, trade_executed):
        # Implement your custom reward logic
        base_reward = super()._calculate_reward(prev_portfolio_value, trade_executed)
        
        # Add custom components
        momentum_bonus = self._calculate_momentum_bonus()
        volatility_penalty = self._calculate_volatility_penalty()
        
        return base_reward + momentum_bonus - volatility_penalty
```

### Custom Strategies
```python
from crypto.rl_trainer import RLStrategy

class CustomRLStrategy(RLStrategy):
    def __init__(self, client, session, notifier, agent):
        super().__init__(client, session, notifier, agent)
        self.custom_parameters = {}
    
    def trading_strategy(self, symbol, data):
        # Custom preprocessing
        processed_data = self.custom_preprocessing(data)
        
        # Use RL agent for decision
        super().trading_strategy(symbol, processed_data)
        
        # Custom post-processing
        self.custom_post_processing()
```

## üìà Best Practices

### Training
1. **Data Quality**: Use clean, validated historical data
2. **Train/Val Split**: Use temporal splits (80/20 or 70/30)
3. **Hyperparameters**: Start with default values, then optimize
4. **Regularization**: Use entropy regularization to prevent overfitting
5. **Ensemble**: Combine multiple agents for robustness

### Backtesting
1. **Out-of-Sample**: Always test on unseen data
2. **Walk-Forward**: Use rolling validation windows
3. **Transaction Costs**: Include realistic fees and slippage
4. **Position Sizing**: Test different capital allocation strategies
5. **Market Regimes**: Test across different market conditions

### Deployment
1. **Paper Trading**: Test with virtual capital first
2. **Position Limits**: Set maximum position sizes
3. **Risk Management**: Implement stop-losses and drawdown limits
4. **Monitoring**: Track performance and model drift
5. **Retraining**: Periodically retrain on new data

## üö® Risk Management

### Built-in Safeguards
- **Drawdown Limits**: Automatic stopping at 10% portfolio loss
- **Position Limits**: Maximum 100% capital allocation
- **Transaction Costs**: Realistic fee modeling
- **Slippage**: Market impact simulation

### Recommended Practices
```python
# Set conservative position sizing
env_config = {
    'max_position_size': 0.2,  # Max 20% per trade
    'transaction_cost': 0.001,  # 0.1% trading fee
    'initial_balance': 10000,   # Starting capital
}

# Implement additional risk checks
class SafeRLStrategy(RLStrategy):
    def trading_strategy(self, symbol, data):
        # Check portfolio heat
        if self.get_portfolio_risk() > 0.15:  # 15% max portfolio risk
            return  # Skip trading
        
        # Check market volatility
        if self.get_market_volatility() > 0.05:  # 5% daily volatility
            return  # Skip in high volatility
        
        super().trading_strategy(symbol, data)
```

## üîç Troubleshooting

### Common Issues

1. **No Data Available**
   ```bash
   # Collect data first
   python crypto/rl_cli.py collect-data BTCUSDT --days-back 365
   ```

2. **Training Instability**
   - Reduce learning rate
   - Increase batch size
   - Add regularization

3. **Poor Performance**
   - Check reward function design
   - Validate data quality
   - Try different algorithms
   - Increase training time

4. **Memory Issues**
   - Reduce lookback window
   - Decrease batch size
   - Use gradient checkpointing

### Debugging
```python
# Enable detailed logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Monitor training
tensorboard --logdir ./models/tensorboard/

# Analyze environment
env = CryptoTradingEnvironment(data)
obs = env.reset()
print(f"Observation shape: {obs.shape}")
print(f"Action space: {env.action_space}")
```

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

- **New Algorithms**: Implement DDPG, SAC, TD3
- **Multi-Asset**: Portfolio optimization strategies  
- **Alternative Data**: Incorporate news, social sentiment
- **Advanced Rewards**: Risk parity, Kelly criterion
- **Model Interpretability**: Attention mechanisms, SHAP values

## üìñ References

- [Stable Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [OpenAI Gym](https://gym.openai.com/)
- [Deep Reinforcement Learning for Trading](https://arxiv.org/abs/2001.06047)
- [FinRL: A Deep Reinforcement Learning Library](https://arxiv.org/abs/2011.09607)

## üìÑ License

This RL trading system extends the existing codebase under the same license terms.

---

**‚ö†Ô∏è Disclaimer**: This software is for educational and research purposes. Cryptocurrency trading involves substantial risk. Past performance does not guarantee future results. Always conduct your own research and consider your risk tolerance before trading.